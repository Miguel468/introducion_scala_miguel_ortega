{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.8:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1569177745412)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "26: error: value plantilla is not a member of Int",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: value plantilla is not a member of Int",
      "       ## plantilla base librerias a importar",
      "          ^",
      "<console>:26: error: not found: value base",
      "       ## plantilla base librerias a importar",
      "                    ^",
      "<console>:26: error: not found: value a",
      "       ## plantilla base librerias a importar",
      "                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "## plantilla base librerias a importar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.8:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1569116814574)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import org.apache.spark.sql.expressions.Window\n",
    "    import org.apache.spark.sql.functions._\n",
    "    import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "    import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"RDDExample\")\n",
    "    .getOrCreate()\n",
    "  import spark.implicits._\n",
    "\n",
    "  val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temas útiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.ERROR) // borra los log.. facilia la lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    val lines = sc.textFile(\"in/word_count.text\")// lectura de un archivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = lines.flatMap(line => line.split(\" \")) // separador ... usa split para parcear y flatmap porque esta \n",
    "                                    //en varios renglones... al final queda un archivo vertical con palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordCounts = words.countByValue()// cuentas de palabra por palabra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Paralelización: es la forma de crear un Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val y var // con val ya no se puede modificar las variables con var es posible modificar las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lista: List[Int] = List(1, 2, 3, 4, 5, 6)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lista = List(1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Int = 2\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista(1) // imprimir un valor de una lista "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8)\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista:::List(7,8) // agregar elementos a la lista es con los ::: tres puntos dobles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: List[Any] = List(List(1, 2, 3, 4, 5, 6), 7, 8)\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista::List(7,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Any = List(1, 2, 3, 4, 5, 6)\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lista::List(7,8))(0) // las encadena pero no une a los elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "33: error: missing argument list for method +: in class List",
     "output_type": "error",
     "traceback": [
      "<console>:33: error: missing argument list for method +: in class List",
      "Unapplied methods are only converted to functions when a function type is expected.",
      "You can make this conversion explicit by writing `$plus$colon _` or `$plus$colon(_)(_)` instead of `$plus$colon`.",
      "       lista.+:List(7)",
      "             ^",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_text: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:32\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_text = sc.parallelize(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Array[Int] = Array(1, 2, 3, 4, 5, 6)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "rdd_text.foreach(println) // no los trae en orden ... los imprime como los encuentra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: Int = 21\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.reduce(_+_) // la suma de todos los elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[Int] = Array(1, 2, 3, 4, 5, 6)\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Long = 6\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Double = 21.0\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lista_con_intervalo: List[scala.collection.immutable.Range.Inclusive] = List(Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24))\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lista_con_intervalo = List(1 to 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DE listas a rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cadena= \"Hola mundo este es un ejemplo de una cadena con map y con flatmap para probar que se separe en un texto individual\"\n",
    "\n",
    "  val cadena_split= cadena.split(\" \")\n",
    "  //cadena_split.foreach(println)\n",
    "val rdd_cade= cadena_split.map(x => (x,x))\n",
    " //rdd_cade.foreach(println)\n",
    "  val new_rdd_cade = sc.parallelize(rdd_cade)\n",
    "  //new_rdd_cade.foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  val COMMA_DELIMITER = \",(?=([^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text.map(x=> if(x.nonEmpty) {x} else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    val text_file= sc.textFile(\"in/prime_nums.text\")\n",
    "    val split_text = text_file.flatMap(x=> x.split(\"\\\\s+\"))\n",
    "    //split_text.foreach(println)\n",
    "    val clean_text = split_text.filter(x=> x.nonEmpty)\n",
    "    //clean_text.foreach(println)\n",
    "val int_text = clean_text.map(x=>x.toInt)\n",
    "    println(int_text.reduce(_+_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " val archivo = sc.textFile(path = \"developercertification-spark/src/main/resources/data/rdd/green_eggs_and_ham.txt\")\n",
    " val archivoFlatMap = archivo.flatMap(x => x.split(\" \")).filter(x => x != \"\" || x != \" \")\n",
    " val archivoMap = archivoFlatMap.map(linea => (linea, 1))\n",
    " val archivoRBK = archivoMap.reduceByKey( _ +_ )\n",
    " archivoRBK.foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
